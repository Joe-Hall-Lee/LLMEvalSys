import gradio as gr
import pandas as pd
import json
from vllm import LLM, SamplingParams
from collections import Counter

# 加载 vllm 模型
llm = LLM(model="/H1/zhouhongli/PORM/output/Llama-3-8B-Instruct-helpsteer2_dpo_0.05_lr_1e-6_epoch_3",
          gpu_memory_utilization=0.5)

# 评估函数（单条数据）


def evaluate(instruction, answer1, answer2):
    prompt = f"""[INST] <<SYS>>
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
<</SYS>>

Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Be neutral, objective, and avoid bias towards either output.
(4) The order of the outputs should not influence your judgment.

Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY "Output (a)" or "Output (b)". Do NOT output any other words.

# Instruction:
{instruction}

# Output (a):
{answer1}

# Output (b):
{answer2}

# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)": [/INST]"""

    # 调用 vllm 生成
    sampling_params = SamplingParams(max_tokens=1024)
    outputs = llm.generate(prompt, sampling_params)
    result = outputs[0].outputs[0].text

    if "Output (a)" in result:
        return "回答 1 更好"
    elif "Output (b)" in result:
        return "回答 2 更好"
    else:
        return "无法确定哪个回答更好"

# 批量评估函数


def evaluate_batch(file, output_path):
    if file.name.endswith('.csv'):
        df = pd.read_csv(file.name)
    elif file.name.endswith('.json'):
        with open(file.name, 'r') as f:
            data = json.load(f)
        df = pd.DataFrame(data)
    else:
        return "仅支持 CSV 或 JSON 格式的文件"

    results = []
    result_counts = Counter()  # 用于统计评估结果的分布

    for _, row in df.iterrows():
        instruction = row['instruction']
        answer1 = row['answer1']
        answer2 = row['answer2']

        prompt = f"""[INST] <<SYS>>
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
<</SYS>>

Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Be neutral, objective, and avoid bias towards either output.
(4) The order of the outputs should not influence your judgment.

Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY "Output (a)" or "Output (b)". Do NOT output any other words.

# Instruction:
{instruction}

# Output (a):
{answer1}

# Output (b):
{answer2}

# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)": [/INST]"""

        # 调用 vllm 进行生成
        sampling_params = SamplingParams(max_tokens=1024)
        outputs = llm.generate(prompt, sampling_params)
        result = outputs[0].outputs[0].text

        if "Output (a)" in result:
            results.append("回答 1 更好")
            result_counts["回答 1 更好"] += 1
        elif "Output (b)" in result:
            results.append("回答 2 更好")
            result_counts["回答 2 更好"] += 1
        else:
            results.append("无法确定哪个回答更好")
            result_counts["无法确定"] += 1

    # 保存评估结果到用户指定路径
    output_df = pd.DataFrame({
        'Instruction': df['instruction'],
        'Answer 1': df['answer1'],
        'Answer 2': df['answer2'],
        'Evaluation Result': results
    })
    try:
        output_df.to_csv(output_path, index=False)
        return f"评估结果已保存到 {output_path}"
    except Exception as e:
        return f"保存文件时出错：{str(e)}"


# 创建 Gradio 页面
with gr.Blocks() as demo:
    with gr.Tab("手动输入"):
        instruction_input = gr.Textbox(label="指令")
        answer1_input = gr.Textbox(label="回答 1")
        answer2_input = gr.Textbox(label="回答 2")
        result_output = gr.Textbox(label="评估结果")

        # 连接手动输入的函数
        evaluate_btn = gr.Button("评估")
        evaluate_btn.click(evaluate, inputs=[
                           instruction_input, answer1_input, answer2_input], outputs=result_output)

    with gr.Tab("批量评估"):
        file_input = gr.File(label="上传 CSV 或 JSON 文件")
        save_path_input = gr.Textbox(
            label="保存路径", placeholder="请输入保存结果的文件路径（如 /path/to/output.csv）")
        batch_result_output = gr.Textbox(label="评估结果")

        # 连接批量评估的函数
        evaluate_btn = gr.Button("开始评估")
        evaluate_btn.click(evaluate_batch, inputs=[
                           file_input, save_path_input], outputs=batch_result_output)

if __name__ == "__main__":
    demo.launch()

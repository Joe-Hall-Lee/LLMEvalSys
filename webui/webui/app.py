from __future__ import annotations
from typing import Iterable
import gradio as gr
from gradio.themes.base import Base
from gradio.themes.utils import colors, fonts, sizes
import time
import pandas as pd
import json
from vllm import LLM, SamplingParams
from collections import Counter

# Load the model
llm = LLM(model="/H1/zhouhongli/POEnhancer/output/Qwen2.5-3B-Instruct-helpsteer2_sft_1e-5_dpo_0.05_lr_1e-6_epoch_3",
          gpu_memory_utilization=0.5)


class Seafoam(Base):
    def __init__(self, *, primary_hue: colors.Color | str = colors.teal,
                 secondary_hue: colors.Color | str = colors.purple,
                 neutral_hue: colors.Color | str = colors.gray,
                 spacing_size: sizes.Size | str = sizes.spacing_md,
                 radius_size: sizes.Size | str = sizes.radius_md,
                 text_size: sizes.Size | str = sizes.text_lg,
                 font: fonts.Font | str | Iterable[fonts.Font | str] = (
                     fonts.GoogleFont("Quicksand"), "ui-sans-serif", "sans-serif"),
                 font_mono: fonts.Font | str | Iterable[fonts.Font | str] = (fonts.GoogleFont("IBM Plex Mono"), "ui-monospace", "monospace")):
        super().__init__(
            primary_hue=primary_hue,
            secondary_hue=secondary_hue,
            neutral_hue=neutral_hue,
            spacing_size=spacing_size,
            radius_size=radius_size,
            text_size=text_size,
            font=font,
            font_mono=font_mono,
        )
        super().set(
            body_background_fill="linear-gradient(135deg, *primary_400, *secondary_400)",
            body_background_fill_dark="linear-gradient(135deg, *primary_700, *secondary_700)",
            button_primary_background_fill="linear-gradient(90deg, *primary_500, *secondary_500)",
            button_primary_background_fill_hover="linear-gradient(90deg, *primary_400, *secondary_400)",
            button_primary_text_color="white",
            button_primary_background_fill_dark="linear-gradient(90deg, *primary_600, *secondary_600)",
            slider_color="*secondary_500",
            slider_color_dark="*secondary_600",
            block_title_text_weight="600",
            block_border_width="2px",
            block_shadow="*shadow_drop_md",
            button_large_padding="24px",
        )


# Initialize the theme
seafoam = Seafoam()


# Evaluation function
def evaluate(instruction, answer1, answer2):
    prompt = f"""[INST] <<SYS>>
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
<</SYS>>

Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Be neutral, objective, and avoid bias towards either output.
(4) The order of the outputs should not influence your judgment.

Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY "Output (a)" or "Output (b)". Do NOT output any other words.

# Instruction:
{instruction}

# Output (a):
{answer1}

# Output (b):
{answer2}

# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)": [/INST]"""

    # Call vllm to generate
    sampling_params = SamplingParams(max_tokens=1024)
    outputs = llm.generate(prompt, sampling_params)
    result = outputs[0].outputs[0].text

    if "Output (a)" in result:
        return "回答 1 更好"
    elif "Output (b)" in result:
        return "回答 2 更好"
    else:
        return "无法确定哪个回答更好"


# Batch evaluation function
def evaluate_batch(file, output_path):
    if file.name.endswith('.csv'):
        df = pd.read_csv(file.name)
    elif file.name.endswith('.json'):
        with open(file.name, 'r') as f:
            data = json.load(f)
        df = pd.DataFrame(data)
    else:
        return "仅支持 CSV 或 JSON 格式的文件"

    results = []
    result_counts = Counter()  # 用于统计评估结果的分布

    for _, row in df.iterrows():
        instruction = row['instruction']
        answer1 = row['answer1']
        answer2 = row['answer2']

        prompt = f"""[INST] <<SYS>>
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
<</SYS>>

Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.

Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Be neutral, objective, and avoid bias towards either output.
(4) The order of the outputs should not influence your judgment.

Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY "Output (a)" or "Output (b)". Do NOT output any other words.

# Instruction:
{instruction}

# Output (a):
{answer1}

# Output (b):
{answer2}

# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)": [/INST]"""

        # Call vllm to generate
        sampling_params = SamplingParams(max_tokens=1024)
        outputs = llm.generate(prompt, sampling_params)
        result = outputs[0].outputs[0].text

        if "Output (a)" in result:
            results.append("回答 1 更好")
            result_counts["回答 1 更好"] += 1
        elif "Output (b)" in result:
            results.append("回答 2 更好")
            result_counts["回答 2 更好"] += 1
        else:
            results.append("无法确定哪个回答更好")
            result_counts["无法确定"] += 1

    # Save the evaluation results to the user-specified path
    output_df = pd.DataFrame({
        'Instruction': df['instruction'],
        'Answer 1': df['answer1'],
        'Answer 2': df['answer2'],
        'Evaluation Result': results
    })
    try:
        output_df.to_csv(output_path, index=False)
        return f"评估结果已保存到 {output_path}"
    except Exception as e:
        return f"保存文件时出错：{str(e)}"


# Create Gradio page with the Seafoam theme
with gr.Blocks(theme=seafoam) as demo:
    gr.Markdown(
        "<div id='header' style='font-size: 28px; color: #ffffff; text-align: center;'>LLM Evaluation Web Application</div>")

    with gr.Tab("Manual Input"):
        instruction_input = gr.Textbox(
            label="Instruction", placeholder="Enter the instruction here...", lines=3)
        answer1_input = gr.Textbox(
            label="Answer 1", placeholder="Enter the first answer here...", lines=3)
        answer2_input = gr.Textbox(
            label="Answer 2", placeholder="Enter the second answer here...", lines=3)
        result_output = gr.Textbox(
            label="Evaluation Result", placeholder="Result will appear here...", lines=2)

        evaluate_btn = gr.Button(
            "Evaluate", elem_id="evaluate_button", size="lg")
        evaluate_btn.click(evaluate, inputs=[
                           instruction_input, answer1_input, answer2_input], outputs=result_output)

    with gr.Tab("Batch Evaluation"):
        file_input = gr.File(label="Upload CSV or JSON File")
        save_path_input = gr.Textbox(
            label="Save Path", placeholder="Enter output file path")
        batch_result_output = gr.Textbox(
            label="Batch Result", placeholder="Results will appear here...", lines=3)

        batch_evaluate_btn = gr.Button("Start Batch Evaluation", size="lg")
        batch_evaluate_btn.click(evaluate_batch, inputs=[
                                 file_input, save_path_input], outputs=batch_result_output)

    gr.Markdown(
        "<div id='footer' style='font-size: 18px; color: #ffffff; text-align: center;'>Designed By Hongli Zhou</div>")

if __name__ == "__main__":
    demo.launch()
